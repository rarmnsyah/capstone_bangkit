{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\GitHub\\capstone_bangkit\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import BertTokenizer, TFAlbertModel, TFAutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from tensorflow.keras.layers import Input, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'AlbertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertModel: ['sop_classifier', 'predictions']\n",
      "- This IS expected if you are initializing TFAlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFAlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFAlbertModel were initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFAlbertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'indobenchmark/indobert-lite-base-p1'\n",
    "# model_name = 'bert-base-cased'\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels = 2)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "indobert_lite = TFAutoModel.from_pretrained(model_name)\n",
    "model = tf.keras.models.load_model('models/model_2023-11-29', custom_objects={\"TFAlbertModel\": TFAlbertModel})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    20944\n",
       "1     6474\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "datapath = os.path.join(cwd, 'datasets')\n",
    "\n",
    "df = pd.read_excel(os.path.join(datapath, 'all_cleaned.xlsx'))\n",
    "df = df[['berita', 'label']]\n",
    "\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.dropna(inplace = True)\n",
    "\n",
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    6474\n",
       "1    6474\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[['berita']]\n",
    "y = df['label']\n",
    "rus = RandomUnderSampler(random_state=1, replacement=True)# fit predictor and target variable\n",
    "X_new, y_new = rus.fit_resample(X,y)\n",
    "y_new.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>berita</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>psi sebut banding pecat viani limardi tolak ad...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12172</th>\n",
       "      <td>tawar koalisi gerindra pks bilang cari teman l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5192</th>\n",
       "      <td>dpr perintah kpu sepakat honor tugas tps naik ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17290</th>\n",
       "      <td>megawati sebut bicara koalisi capres lihat din...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10955</th>\n",
       "      <td>dana milu capai rp triliun jokowi minta detail...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27427</th>\n",
       "      <td>raja salman arab saudi bawa orang orang sudah ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27428</th>\n",
       "      <td>hehe selalu senyum lihat tingkah laku pak joko...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27429</th>\n",
       "      <td>pak jokowi jadi walikota periode pertama solo ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27430</th>\n",
       "      <td>hari rabu nilai tukar rupiah puruk hingga semp...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27431</th>\n",
       "      <td>kita tolak desember acara pastur monas majlis ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12948 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  berita  label\n",
       "235    psi sebut banding pecat viani limardi tolak ad...      0\n",
       "12172  tawar koalisi gerindra pks bilang cari teman l...      0\n",
       "5192   dpr perintah kpu sepakat honor tugas tps naik ...      0\n",
       "17290  megawati sebut bicara koalisi capres lihat din...      0\n",
       "10955  dana milu capai rp triliun jokowi minta detail...      0\n",
       "...                                                  ...    ...\n",
       "27427  raja salman arab saudi bawa orang orang sudah ...      1\n",
       "27428  hehe selalu senyum lihat tingkah laku pak joko...      1\n",
       "27429  pak jokowi jadi walikota periode pertama solo ...      1\n",
       "27430  hari rabu nilai tukar rupiah puruk hingga semp...      1\n",
       "27431  kita tolak desember acara pastur monas majlis ...      1\n",
       "\n",
       "[12948 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new = X_new\n",
    "df_new['label'] = y_new\n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df_new, test_size=0.2, random_state=42,\n",
    "                                     stratify=df_new['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_albert_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " albert (TFAlbertMainLayer)  multiple                  11683584  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11683584 (44.57 MB)\n",
      "Trainable params: 11683584 (44.57 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "indobert_lite.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TFAlbertMainLayer' object has no attribute 'embedding'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\LENOVO\\GitHub\\capstone_bangkit\\test_pruning_model.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/LENOVO/GitHub/capstone_bangkit/test_pruning_model.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m indobert_lite\u001b[39m.\u001b[39;49mlayers[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49membedding\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TFAlbertMainLayer' object has no attribute 'embedding'"
     ]
    }
   ],
   "source": [
    "indobert_lite.layers[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "import re\n",
    "\n",
    "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
    "\n",
    "pruning_schedule = sparsity.PolynomialDecay(\n",
    "                        initial_sparsity=0.0, final_sparsity=0.5,\n",
    "                        begin_step=0, end_step=12, frequency=100)\n",
    "\n",
    "pruned_model = tf.keras.Sequential()\n",
    "\n",
    "for layer in indobert_lite.layers:\n",
    "    if(re.match(r\"conv_pw_\\d+$\", layer.name)):\n",
    "         pruned_model.add(sparsity.prune_low_magnitude(\n",
    "            layer,\n",
    "            pruning_schedule,\n",
    "            block_size=(1,1)\n",
    "         ))\n",
    "    else:\n",
    "        pruned_model.add(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\LENOVO\\GitHub\\capstone_bangkit\\test_pruning_model.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/LENOVO/GitHub/capstone_bangkit/test_pruning_model.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m max_len \u001b[39m=\u001b[39m \u001b[39m70\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/LENOVO/GitHub/capstone_bangkit/test_pruning_model.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m X_train \u001b[39m=\u001b[39m tokenizer(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/LENOVO/GitHub/capstone_bangkit/test_pruning_model.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     text\u001b[39m=\u001b[39;49mdf_train[\u001b[39m'\u001b[39;49m\u001b[39mberita\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mtolist(),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/LENOVO/GitHub/capstone_bangkit/test_pruning_model.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/LENOVO/GitHub/capstone_bangkit/test_pruning_model.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_len,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/LENOVO/GitHub/capstone_bangkit/test_pruning_model.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     truncation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/LENOVO/GitHub/capstone_bangkit/test_pruning_model.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     padding\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/LENOVO/GitHub/capstone_bangkit/test_pruning_model.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     return_tensors\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtf\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/LENOVO/GitHub/capstone_bangkit/test_pruning_model.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/LENOVO/GitHub/capstone_bangkit/test_pruning_model.ipynb#X10sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/LENOVO/GitHub/capstone_bangkit/test_pruning_model.ipynb#X10sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/LENOVO/GitHub/capstone_bangkit/test_pruning_model.ipynb#X10sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/LENOVO/GitHub/capstone_bangkit/test_pruning_model.ipynb#X10sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m X_test \u001b[39m=\u001b[39m tokenizer(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/LENOVO/GitHub/capstone_bangkit/test_pruning_model.ipynb#X10sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     text\u001b[39m=\u001b[39mdf_test[\u001b[39m'\u001b[39m\u001b[39mberita\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mtolist(),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/LENOVO/GitHub/capstone_bangkit/test_pruning_model.ipynb#X10sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     add_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/LENOVO/GitHub/capstone_bangkit/test_pruning_model.ipynb#X10sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/LENOVO/GitHub/capstone_bangkit/test_pruning_model.ipynb#X10sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\GitHub\\capstone_bangkit\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2798\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2796\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   2797\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 2798\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_one(text\u001b[39m=\u001b[39mtext, text_pair\u001b[39m=\u001b[39mtext_pair, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mall_kwargs)\n\u001b[0;32m   2799\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2800\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\GitHub\\capstone_bangkit\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2884\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2879\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2880\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatch length of `text`: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text)\u001b[39m}\u001b[39;00m\u001b[39m does not match batch length of `text_pair`:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2881\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text_pair)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2882\u001b[0m         )\n\u001b[0;32m   2883\u001b[0m     batch_text_or_text_pairs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(text, text_pair)) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m text\n\u001b[1;32m-> 2884\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_encode_plus(\n\u001b[0;32m   2885\u001b[0m         batch_text_or_text_pairs\u001b[39m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   2886\u001b[0m         add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2887\u001b[0m         padding\u001b[39m=\u001b[39mpadding,\n\u001b[0;32m   2888\u001b[0m         truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   2889\u001b[0m         max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m   2890\u001b[0m         stride\u001b[39m=\u001b[39mstride,\n\u001b[0;32m   2891\u001b[0m         is_split_into_words\u001b[39m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2892\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2893\u001b[0m         return_tensors\u001b[39m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2894\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2895\u001b[0m         return_attention_mask\u001b[39m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2896\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2897\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2898\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2899\u001b[0m         return_length\u001b[39m=\u001b[39mreturn_length,\n\u001b[0;32m   2900\u001b[0m         verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m   2901\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2902\u001b[0m     )\n\u001b[0;32m   2903\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2904\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_plus(\n\u001b[0;32m   2905\u001b[0m         text\u001b[39m=\u001b[39mtext,\n\u001b[0;32m   2906\u001b[0m         text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2922\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2923\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\GitHub\\capstone_bangkit\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3075\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   3065\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   3066\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   3067\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[0;32m   3068\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3072\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   3073\u001b[0m )\n\u001b[1;32m-> 3075\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_encode_plus(\n\u001b[0;32m   3076\u001b[0m     batch_text_or_text_pairs\u001b[39m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   3077\u001b[0m     add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   3078\u001b[0m     padding_strategy\u001b[39m=\u001b[39mpadding_strategy,\n\u001b[0;32m   3079\u001b[0m     truncation_strategy\u001b[39m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   3080\u001b[0m     max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m   3081\u001b[0m     stride\u001b[39m=\u001b[39mstride,\n\u001b[0;32m   3082\u001b[0m     is_split_into_words\u001b[39m=\u001b[39mis_split_into_words,\n\u001b[0;32m   3083\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   3084\u001b[0m     return_tensors\u001b[39m=\u001b[39mreturn_tensors,\n\u001b[0;32m   3085\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   3086\u001b[0m     return_attention_mask\u001b[39m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   3087\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   3088\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   3089\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   3090\u001b[0m     return_length\u001b[39m=\u001b[39mreturn_length,\n\u001b[0;32m   3091\u001b[0m     verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m   3092\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   3093\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\GitHub\\capstone_bangkit\\venv\\lib\\site-packages\\transformers\\tokenization_utils.py:803\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    801\u001b[0m     ids, pair_ids \u001b[39m=\u001b[39m ids_or_pair_ids\n\u001b[1;32m--> 803\u001b[0m first_ids \u001b[39m=\u001b[39m get_input_ids(ids)\n\u001b[0;32m    804\u001b[0m second_ids \u001b[39m=\u001b[39m get_input_ids(pair_ids) \u001b[39mif\u001b[39;00m pair_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    805\u001b[0m input_ids\u001b[39m.\u001b[39mappend((first_ids, second_ids))\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\GitHub\\capstone_bangkit\\venv\\lib\\site-packages\\transformers\\tokenization_utils.py:770\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus.<locals>.get_input_ids\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m    768\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_input_ids\u001b[39m(text):\n\u001b[0;32m    769\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(text, \u001b[39mstr\u001b[39m):\n\u001b[1;32m--> 770\u001b[0m         tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenize(text, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    771\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconvert_tokens_to_ids(tokens)\n\u001b[0;32m    772\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(text, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(text) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(text[\u001b[39m0\u001b[39m], \u001b[39mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\GitHub\\capstone_bangkit\\venv\\lib\\site-packages\\transformers\\tokenization_utils.py:581\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize\u001b[1;34m(self, text, **kwargs)\u001b[0m\n\u001b[0;32m    579\u001b[0m     no_split_token \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_added_tokens_encoder\u001b[39m.\u001b[39mkeys()  \u001b[39m# don't split on any of the added tokens\u001b[39;00m\n\u001b[0;32m    580\u001b[0m     \u001b[39m# \"This is something<special_token_1>  else\"\u001b[39;00m\n\u001b[1;32m--> 581\u001b[0m     tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokens_trie\u001b[39m.\u001b[39;49msplit(text)\n\u001b[0;32m    583\u001b[0m \u001b[39m# [\"This is something\", \"<special_token_1>\", \"  else\"]\u001b[39;00m\n\u001b[0;32m    584\u001b[0m \u001b[39mfor\u001b[39;00m i, token \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tokens):\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\GitHub\\capstone_bangkit\\venv\\lib\\site-packages\\transformers\\tokenization_utils.py:139\u001b[0m, in \u001b[0;36mTrie.split\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39m# Main loop, Giving this algorithm O(n) complexity\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[39mfor\u001b[39;00m current, current_char \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(text):\n\u001b[1;32m--> 139\u001b[0m     \u001b[39mif\u001b[39;00m skip \u001b[39mand\u001b[39;00m current \u001b[39m<\u001b[39m skip:\n\u001b[0;32m    140\u001b[0m         \u001b[39m# Prevents the lookahead for matching twice\u001b[39;00m\n\u001b[0;32m    141\u001b[0m         \u001b[39m# like extra_id_100 and id_100\u001b[39;00m\n\u001b[0;32m    142\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     \u001b[39m# This will track every state\u001b[39;00m\n\u001b[0;32m    145\u001b[0m     \u001b[39m# that stop matching, we need to stop tracking them.\u001b[39;00m\n\u001b[0;32m    146\u001b[0m     \u001b[39m# If we look at \"lowball\", we're going to match \"l\" (add it to states), \"o\", \"w\", then\u001b[39;00m\n\u001b[0;32m    147\u001b[0m     \u001b[39m# fail on \"b\", we need to remove 0 from the valid states.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_len = 70\n",
    "\n",
    "X_train = tokenizer(\n",
    "    text=df_train['berita'].tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=max_len,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors='tf',\n",
    "    return_token_type_ids=False,\n",
    "    return_attention_mask=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "X_test = tokenizer(\n",
    "    text=df_test['berita'].tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=max_len,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors='tf',\n",
    "    return_token_type_ids=False,\n",
    "    return_attention_mask=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
